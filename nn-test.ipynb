{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from itertools import product\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model import utils\n",
    "from tensorflow.python.saved_model import signature_constants\n",
    "from tensorflow.python.saved_model import signature_def_utils\n",
    "from tensorflow.python.saved_model.builder_impl import SavedModelBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_product(dicts):\n",
    "    result = []\n",
    "    for d in dicts:\n",
    "        result += list((dict(zip(d, x))) for x in product(*d.values()))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_num_in_window(frequency, window_size_ms):\n",
    "    return int(window_size_ms * frequency / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emg_data_windowing(data, window_size):\n",
    "    data_win = np.copy(data)\n",
    "    data_x = data_win[:,:-1]\n",
    "    data_y = data_win[:,-1]\n",
    "    n, m = data_x.shape\n",
    "    size = n * m\n",
    "    residual_rows_num =  n % window_size\n",
    "    if residual_rows_num != 0:\n",
    "        data_x = data_x[:-residual_rows_num,:]\n",
    "        data_y = data_y[:-residual_rows_num]\n",
    "    data_x = data_x.reshape((-1, m * window_size))\n",
    "    \n",
    "    data_y = data_y.reshape((-1, window_size))\n",
    "    data_y = np.array(list(map(np.mean, data_y)))\n",
    "    \n",
    "    mixed_classes_idxs = np.where(data_y % 1 != 0)\n",
    "    \n",
    "    data_win = np.c_[data_x, data_y]\n",
    "    data_win = np.delete(data_win, mixed_classes_idxs, 0)\n",
    "    \n",
    "    return data_win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_emg(data_path):\n",
    "    sessions_csv = []\n",
    "    for path, _, files in os.walk(data_path):\n",
    "        for name in files:\n",
    "            sessions_csv.append(os.path.join(path, name))\n",
    "\n",
    "    data = pd.concat([pd.read_csv(file, header = None) for file in sessions_csv]).values\n",
    "    print('input shape', data.shape)\n",
    "    \n",
    "    # reshape data\n",
    "    # one column - one channel\n",
    "    data_x = data[:,:-1]\n",
    "    data_y = data[:,-1]\n",
    "    data_x = data_x.reshape((-1, 8))\n",
    "    data_y = data_y.repeat(8)\n",
    "    data_y = data_y.reshape((-1,1))\n",
    "    data = np.concatenate((data_x, data_y), axis=1)\n",
    "    print('result shape: ', data.shape)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nitime.algorithms.autoregressive import AR_est_LD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def autoregression_coefficients(emg, order):\n",
    "    coef = AR_est_LD(emg, order=order)[0]\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def integrated_absolute_value(segment):\n",
    "    return sum([abs(s) for s in segment])\n",
    "\n",
    "def mean_absolute_value(segment):\n",
    "    return sum([abs(s) for s in segment])/len(segment)\n",
    "\n",
    "def waveform_length(segment):\n",
    "    n = len(segment)\n",
    "    wl = 0\n",
    "    for i in range(1, n):\n",
    "        wl += abs(segment[i] - segment[i-1])\n",
    "    return wl\n",
    "\n",
    "def zero_crossing(segment):\n",
    "    n = len(segment)\n",
    "    zc = 0\n",
    "    for i in range(n - 1):\n",
    "        if segment[i] * segment[i+1] < 0:\n",
    "            zc += 1\n",
    "    return zc\n",
    "\n",
    "def slope_sign_changes(segment):\n",
    "    n = len(segment)\n",
    "    ssc = 0\n",
    "    for i in range(1, n-1):\n",
    "        if segment[i-1] < segment[i] and segment[i] > segment[i+1] or segment[i-1] > segment[i] and segment[i] < segment[i+1]:\n",
    "            ssc += 1\n",
    "    return ssc\n",
    "\n",
    "def root_mean_square(segment):\n",
    "    return math.sqrt(sum([s*s for s in segment])/len(segment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(data_x, channels_num, ar_features=True):\n",
    "    n, m = data_x.shape\n",
    "    features = []\n",
    "    \n",
    "    for channel in range(channels_num):\n",
    "        channel_features = []\n",
    "        \n",
    "        # Calculate MAV, ZC, SSC, WL, RMS features\n",
    "        channel_features.append(list(map(mean_absolute_value, data_x[:,channel::channels_num])))\n",
    "        channel_features.append(list(map(waveform_length, data_x[:,channel::channels_num])))\n",
    "        channel_features.append(list(map(zero_crossing, data_x[:,channel::channels_num])))\n",
    "        channel_features.append(list(map(slope_sign_changes, data_x[:,channel::channels_num])))\n",
    "        channel_features.append(list(map(root_mean_square, data_x[:,channel::channels_num])))\n",
    "        \n",
    "        if ar_features:\n",
    "            # calculate AR6 coefficients\n",
    "            ar_order = 6\n",
    "            ar_coef = np.array(list(map(lambda x: autoregression_coefficients(x, ar_order), data_x[:,channel::channels_num])))\n",
    "            channel_features += ar_coef.transpose().tolist()\n",
    "        \n",
    "        features += channel_features\n",
    "    \n",
    "    return np.array(features).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/gestures-9/p001\n",
      "data/gestures-9/p001\\session1\n",
      "input shape (2250, 65)\n",
      "result shape:  (18000, 9)\n",
      "data/gestures-9/p001\\session2\n",
      "input shape (2250, 65)\n",
      "result shape:  (18000, 9)\n",
      "data/gestures-9/p001\\session3\n",
      "input shape (2250, 65)\n",
      "result shape:  (18000, 9)\n",
      "\n",
      "data/gestures-9/p002\n",
      "data/gestures-9/p002\\session1\n",
      "input shape (2251, 65)\n",
      "result shape:  (18008, 9)\n",
      "data/gestures-9/p002\\session2\n",
      "input shape (2251, 65)\n",
      "result shape:  (18008, 9)\n",
      "data/gestures-9/p002\\session3\n",
      "input shape (2251, 65)\n",
      "result shape:  (18008, 9)\n",
      "\n",
      "data/gestures-9/p003\n",
      "data/gestures-9/p003\\session1\n",
      "input shape (2251, 65)\n",
      "result shape:  (18008, 9)\n",
      "data/gestures-9/p003\\session2\n",
      "input shape (2251, 65)\n",
      "result shape:  (18008, 9)\n",
      "data/gestures-9/p003\\session3\n",
      "input shape (2251, 65)\n",
      "result shape:  (18008, 9)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "datasets_path = 'data/gestures-9/'\n",
    "datasets = []\n",
    "for dataset_name in list(os.walk(datasets_path))[0][1]:\n",
    "    dataset_path = datasets_path + dataset_name\n",
    "    print(dataset_path)\n",
    "    session_names = list(os.walk(dataset_path))[0][1]\n",
    "    sessions = []\n",
    "    for session_name in session_names:\n",
    "        current_session = os.path.join(dataset_path, session_name)\n",
    "        print(current_session)\n",
    "        sessions.append(read_emg(current_session))\n",
    "    datasets.append(sessions)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_datasets = []\n",
    "for dataset in datasets:\n",
    "    preprocessed_sessions = []\n",
    "    for session in dataset:\n",
    "        window_samples = samples_num_in_window(200, 200)\n",
    "        session_win = emg_data_windowing(session, window_samples)\n",
    "\n",
    "        session_X = session_win[:,:-1]\n",
    "        session_y = session_win[:,-1].astype('int')\n",
    "\n",
    "        session_features_X = calculate_features(session_X, 8, True)\n",
    "        # Concatenate X and y\n",
    "        session_features = np.c_[session_features_X, session_y]\n",
    "        preprocessed_sessions.append(session_features)\n",
    "    preprocessed_datasets.append(preprocessed_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 89)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_datasets[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.concatenate(preprocessed_datasets[0][0:2])\n",
    "test = preprocessed_datasets[0][2]\n",
    "\n",
    "Ytrain = train[:,-1].astype('int')\n",
    "Xtrain = train[:,:-1]\n",
    "\n",
    "Ytest = test[:,-1].astype('int')\n",
    "Xtest = test[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-390edbae92f2>:49: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.11333333333333333\n",
      "classification_rate: 0.13844444444444445\n",
      "classification_rate: 0.15911111111111112\n",
      "classification_rate: 0.12311111111111112\n",
      "classification_rate: 0.11066666666666666\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.088\n",
      "classification_rate: 0.18711111111111112\n",
      "classification_rate: 0.22511111111111112\n",
      "classification_rate: 0.2604444444444444\n",
      "classification_rate: 0.3042222222222222\n",
      "classification_rate: 0.3008888888888889\n",
      "classification_rate: 0.30333333333333334\n",
      "classification_rate: 0.306\n",
      "classification_rate: 0.41488888888888886\n",
      "classification_rate: 0.4928888888888889\n",
      "classification_rate: 0.5104444444444445\n",
      "classification_rate: 0.5388888888888889\n",
      "classification_rate: 0.5991111111111111\n",
      "classification_rate: 0.6168888888888889\n",
      "classification_rate: 0.6531111111111111\n",
      "classification_rate: 0.6757777777777778\n",
      "classification_rate: 0.6968888888888889\n",
      "Test Set classification rate: 0.6382222222222222\n"
     ]
    }
   ],
   "source": [
    "NUM_READS = 1 # each row contains 1 read\n",
    "NUM_SENSORS = 88 # each read has 8 sensors(channels) * 5 features\n",
    "\n",
    "D = NUM_SENSORS * NUM_READS # number of input features\n",
    "M1 = 25 # first layer number of nodes, relatively arbitrarily chosen\n",
    "M2 = 15 # second hidden layer number of nodes, relatively arbitrarily chosen\n",
    "M3 = 10 # third hidden layer number of nodes, relatively arbitrarily chosen\n",
    "\n",
    "K = 9 # output layer nodes or number of classes\n",
    "\n",
    "N = len(Ytrain)\n",
    "T = np.zeros((N, K))\n",
    "for i in range(N):\n",
    "    T[i, Ytrain[i]] = 1 # this creates an indicator/dummy variable matrix for the output layer. We need to do this for\n",
    "# two reasons. 1) it creates an NxK matrix that will be broadcastable with the predictions generated from the forward\n",
    "# function and used in the cost function. 2) when we argmax the predictions, it will turn into a matrix NxK of values only\n",
    "# either 1 or 0 which can directly be compared with T to test the accuracy\n",
    "\n",
    "\n",
    "def initialize_weights_and_biases(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "def feed_forward(W4, W3, W2, W1, b4, b3, b2, b1, X):\n",
    "    Z1 = tf.matmul(X, W1)\n",
    "#     Z1 = tf.nn.dropout(Z1, 0.9) #check TFLite support\n",
    "    Z1 = tf.nn.relu(Z1 + b1)\n",
    "\n",
    "    Z2 = tf.matmul(Z1, W2)\n",
    "#     Z2 = tf.nn.dropout(Z2, 0.9) #check TFLite support\n",
    "    Z2 = tf.nn.relu(Z2 + b2)\n",
    "\n",
    "    Z3 = tf.nn.relu(tf.matmul(Z2, W3) + b3)\n",
    "    return tf.matmul(Z3, W4) + b4, Z3\n",
    "\n",
    "tfX = tf.placeholder(tf.float32, [None, D]) # creates placeholder variables without actually assigning values to them yet\n",
    "tfY = tf.placeholder(tf.float32, [None, K]) # None means it can take any size N total number of instances\n",
    "\n",
    "\n",
    "W1 = initialize_weights_and_biases([D, M1])\n",
    "W2 = initialize_weights_and_biases([M1, M2])\n",
    "W3 = initialize_weights_and_biases([M2, M3])\n",
    "W4 = initialize_weights_and_biases([M3, K])\n",
    "b1 = initialize_weights_and_biases([M1])\n",
    "b2 = initialize_weights_and_biases([M2])\n",
    "b3 = initialize_weights_and_biases([M3])\n",
    "b4 = initialize_weights_and_biases([K])\n",
    "\n",
    "pY_given_X, mid_layer = feed_forward(W4, W3, W2, W1, b4, b3, b2, b1, tfX)\n",
    "y_max = tf.argmax(pY_given_X, dimension=1)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    labels = tfY, logits = pY_given_X))\n",
    "\n",
    "train_model = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "predict_output = tf.argmax(pY_given_X, 1) # 1 refers to axis = 1, meaning it does argmax on each instance n\n",
    "\n",
    "session = tf.Session()\n",
    "initializer = tf.global_variables_initializer()\n",
    "session.run(initializer)\n",
    "\n",
    "for i in range(8000):\n",
    "    session.run(train_model, feed_dict = {tfX: Xtrain, tfY: T})\n",
    "    pred = session.run(predict_output, feed_dict = {tfX:Xtrain, tfY:T})\n",
    "    if i % 250 == 0:\n",
    "        print(\"classification_rate: {}\".format(np.mean(Ytrain == pred)))\n",
    "\n",
    "# Test Set evaluation\n",
    "Ntest = len(Ytest)\n",
    "Ttest = np.zeros((Ntest, K)) # test set indicator matrix\n",
    "for i in range(Ntest):\n",
    "    Ttest[i, Ytest[i]] = 1\n",
    "\n",
    "predtest = session.run(predict_output, feed_dict = {tfX: Xtest, tfY: Ttest})\n",
    "print(\"Test Set classification rate: {}\".format(np.mean(Ytest == predtest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.27666666666666667\n",
      "classification_rate: 0.4022222222222222\n",
      "classification_rate: 0.7244444444444444\n",
      "classification_rate: 0.8066666666666666\n",
      "classification_rate: 0.8755555555555555\n",
      "classification_rate: 0.8711111111111111\n",
      "classification_rate: 0.9066666666666666\n",
      "classification_rate: 0.9188888888888889\n",
      "classification_rate: 0.8811111111111111\n",
      "classification_rate: 0.7866666666666666\n",
      "classification_rate: 0.9188888888888889\n",
      "classification_rate: 0.9311111111111111\n",
      "classification_rate: 0.7577777777777778\n",
      "classification_rate: 0.7933333333333333\n",
      "classification_rate: 0.8655555555555555\n",
      "classification_rate: 0.8911111111111111\n",
      "classification_rate: 0.9122222222222223\n",
      "classification_rate: 0.9188888888888889\n",
      "classification_rate: 0.9188888888888889\n",
      "classification_rate: 0.9188888888888889\n",
      "classification_rate: 0.9411111111111111\n",
      "classification_rate: 0.9333333333333333\n",
      "classification_rate: 0.9355555555555556\n",
      "classification_rate: 0.8377777777777777\n",
      "classification_rate: 0.9022222222222223\n",
      "classification_rate: 0.8044444444444444\n",
      "classification_rate: 0.9322222222222222\n",
      "classification_rate: 0.9411111111111111\n",
      "classification_rate: 0.9411111111111111\n",
      "Test Set classification rate: 0.7888888888888889\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.17888888888888888\n",
      "classification_rate: 0.18444444444444444\n",
      "classification_rate: 0.3933333333333333\n",
      "classification_rate: 0.7355555555555555\n",
      "classification_rate: 0.8522222222222222\n",
      "classification_rate: 0.8633333333333333\n",
      "classification_rate: 0.8833333333333333\n",
      "classification_rate: 0.8988888888888888\n",
      "classification_rate: 0.9111111111111111\n",
      "classification_rate: 0.8933333333333333\n",
      "classification_rate: 0.9277777777777778\n",
      "classification_rate: 0.9155555555555556\n",
      "classification_rate: 0.9166666666666666\n",
      "classification_rate: 0.9266666666666666\n",
      "classification_rate: 0.9288888888888889\n",
      "classification_rate: 0.9311111111111111\n",
      "classification_rate: 0.93\n",
      "classification_rate: 0.9322222222222222\n",
      "classification_rate: 0.9344444444444444\n",
      "classification_rate: 0.9355555555555556\n",
      "classification_rate: 0.9377777777777778\n",
      "classification_rate: 0.9388888888888889\n",
      "classification_rate: 0.9422222222222222\n",
      "classification_rate: 0.9433333333333334\n",
      "classification_rate: 0.9477777777777778\n",
      "classification_rate: 0.9477777777777778\n",
      "classification_rate: 0.95\n",
      "classification_rate: 0.9488888888888889\n",
      "classification_rate: 0.9477777777777778\n",
      "Test Set classification rate: 0.7311111111111112\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.22777777777777777\n",
      "classification_rate: 0.3188888888888889\n",
      "classification_rate: 0.4444444444444444\n",
      "classification_rate: 0.6833333333333333\n",
      "classification_rate: 0.7133333333333334\n",
      "classification_rate: 0.7111111111111111\n",
      "classification_rate: 0.85\n",
      "classification_rate: 0.8688888888888889\n",
      "classification_rate: 0.8777777777777778\n",
      "classification_rate: 0.8966666666666666\n",
      "classification_rate: 0.8988888888888888\n",
      "classification_rate: 0.9055555555555556\n",
      "classification_rate: 0.9\n",
      "classification_rate: 0.8988888888888888\n",
      "classification_rate: 0.9133333333333333\n",
      "classification_rate: 0.9122222222222223\n",
      "classification_rate: 0.9188888888888889\n",
      "classification_rate: 0.92\n",
      "classification_rate: 0.9244444444444444\n",
      "classification_rate: 0.8822222222222222\n",
      "classification_rate: 0.8588888888888889\n",
      "classification_rate: 0.9333333333333333\n",
      "classification_rate: 0.93\n",
      "classification_rate: 0.9433333333333334\n",
      "classification_rate: 0.9344444444444444\n",
      "classification_rate: 0.9355555555555556\n",
      "classification_rate: 0.9355555555555556\n",
      "classification_rate: 0.9344444444444444\n",
      "classification_rate: 0.9388888888888889\n",
      "Test Set classification rate: 0.7533333333333333\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1988888888888889\n",
      "classification_rate: 0.31666666666666665\n",
      "classification_rate: 0.5488888888888889\n",
      "classification_rate: 0.74\n",
      "classification_rate: 0.8055555555555556\n",
      "classification_rate: 0.8266666666666667\n",
      "classification_rate: 0.8988888888888888\n",
      "classification_rate: 0.9088888888888889\n",
      "classification_rate: 0.8566666666666667\n",
      "classification_rate: 0.9\n",
      "classification_rate: 0.9055555555555556\n",
      "classification_rate: 0.8522222222222222\n",
      "classification_rate: 0.7822222222222223\n",
      "classification_rate: 0.8988888888888888\n",
      "classification_rate: 0.9011111111111111\n",
      "classification_rate: 0.9222222222222223\n",
      "classification_rate: 0.9311111111111111\n",
      "classification_rate: 0.6266666666666667\n",
      "classification_rate: 0.7344444444444445\n",
      "classification_rate: 0.81\n",
      "classification_rate: 0.7988888888888889\n",
      "classification_rate: 0.8266666666666667\n",
      "classification_rate: 0.8711111111111111\n",
      "classification_rate: 0.8977777777777778\n",
      "classification_rate: 0.9333333333333333\n",
      "classification_rate: 0.9277777777777778\n",
      "classification_rate: 0.8633333333333333\n",
      "classification_rate: 0.6888888888888889\n",
      "classification_rate: 0.9188888888888889\n",
      "Test Set classification rate: 0.82\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.22555555555555556\n",
      "classification_rate: 0.45222222222222225\n",
      "classification_rate: 0.7877777777777778\n",
      "classification_rate: 0.7788888888888889\n",
      "classification_rate: 0.88\n",
      "classification_rate: 0.8955555555555555\n",
      "classification_rate: 0.9188888888888889\n",
      "classification_rate: 0.8477777777777777\n",
      "classification_rate: 0.8922222222222222\n",
      "classification_rate: 0.9355555555555556\n",
      "classification_rate: 0.9266666666666666\n",
      "classification_rate: 0.9288888888888889\n",
      "classification_rate: 0.9322222222222222\n",
      "classification_rate: 0.9388888888888889\n",
      "classification_rate: 0.94\n",
      "classification_rate: 0.9411111111111111\n",
      "classification_rate: 0.9411111111111111\n",
      "classification_rate: 0.9422222222222222\n",
      "classification_rate: 0.9433333333333334\n",
      "classification_rate: 0.9477777777777778\n",
      "classification_rate: 0.9477777777777778\n",
      "classification_rate: 0.9555555555555556\n",
      "classification_rate: 0.9488888888888889\n",
      "classification_rate: 0.9544444444444444\n",
      "classification_rate: 0.9566666666666667\n",
      "classification_rate: 0.9577777777777777\n",
      "classification_rate: 0.9544444444444444\n",
      "classification_rate: 0.9511111111111111\n",
      "classification_rate: 0.9555555555555556\n",
      "Test Set classification rate: 0.82\n",
      "mean: 0.7826666666666666\n",
      "-------------------------------------\n",
      "\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.15\n",
      "classification_rate: 0.2722222222222222\n",
      "classification_rate: 0.2911111111111111\n",
      "classification_rate: 0.5211111111111111\n",
      "classification_rate: 0.5444444444444444\n",
      "classification_rate: 0.6488888888888888\n",
      "classification_rate: 0.63\n",
      "classification_rate: 0.6955555555555556\n",
      "classification_rate: 0.73\n",
      "classification_rate: 0.7677777777777778\n",
      "classification_rate: 0.7422222222222222\n",
      "classification_rate: 0.8166666666666667\n",
      "classification_rate: 0.7822222222222223\n",
      "classification_rate: 0.8255555555555556\n",
      "classification_rate: 0.8055555555555556\n",
      "classification_rate: 0.8188888888888889\n",
      "classification_rate: 0.8233333333333334\n",
      "classification_rate: 0.8077777777777778\n",
      "classification_rate: 0.8477777777777777\n",
      "classification_rate: 0.8122222222222222\n",
      "classification_rate: 0.8488888888888889\n",
      "classification_rate: 0.8644444444444445\n",
      "classification_rate: 0.8733333333333333\n",
      "classification_rate: 0.8588888888888889\n",
      "classification_rate: 0.8588888888888889\n",
      "classification_rate: 0.8588888888888889\n",
      "classification_rate: 0.8811111111111111\n",
      "classification_rate: 0.8766666666666667\n",
      "classification_rate: 0.87\n",
      "classification_rate: 0.8744444444444445\n",
      "classification_rate: 0.8855555555555555\n",
      "Test Set classification rate: 0.6333333333333333\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.2388888888888889\n",
      "classification_rate: 0.22555555555555556\n",
      "classification_rate: 0.41555555555555557\n",
      "classification_rate: 0.4811111111111111\n",
      "classification_rate: 0.4822222222222222\n",
      "classification_rate: 0.6155555555555555\n",
      "classification_rate: 0.6488888888888888\n",
      "classification_rate: 0.6644444444444444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_rate: 0.7144444444444444\n",
      "classification_rate: 0.64\n",
      "classification_rate: 0.79\n",
      "classification_rate: 0.7566666666666667\n",
      "classification_rate: 0.8166666666666667\n",
      "classification_rate: 0.8222222222222222\n",
      "classification_rate: 0.8133333333333334\n",
      "classification_rate: 0.8344444444444444\n",
      "classification_rate: 0.8166666666666667\n",
      "classification_rate: 0.7422222222222222\n",
      "classification_rate: 0.8455555555555555\n",
      "classification_rate: 0.8244444444444444\n",
      "classification_rate: 0.8311111111111111\n",
      "classification_rate: 0.8455555555555555\n",
      "classification_rate: 0.8555555555555555\n",
      "classification_rate: 0.8344444444444444\n",
      "classification_rate: 0.8666666666666667\n",
      "classification_rate: 0.8622222222222222\n",
      "classification_rate: 0.86\n",
      "classification_rate: 0.8477777777777777\n",
      "classification_rate: 0.8711111111111111\n",
      "classification_rate: 0.8688888888888889\n",
      "Test Set classification rate: 0.6377777777777778\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.22777777777777777\n",
      "classification_rate: 0.29888888888888887\n",
      "classification_rate: 0.2866666666666667\n",
      "classification_rate: 0.4911111111111111\n",
      "classification_rate: 0.6055555555555555\n",
      "classification_rate: 0.6744444444444444\n",
      "classification_rate: 0.6433333333333333\n",
      "classification_rate: 0.7244444444444444\n",
      "classification_rate: 0.7522222222222222\n",
      "classification_rate: 0.7722222222222223\n",
      "classification_rate: 0.7833333333333333\n",
      "classification_rate: 0.6733333333333333\n",
      "classification_rate: 0.8166666666666667\n",
      "classification_rate: 0.7844444444444445\n",
      "classification_rate: 0.8277777777777777\n",
      "classification_rate: 0.7744444444444445\n",
      "classification_rate: 0.8377777777777777\n",
      "classification_rate: 0.8233333333333334\n",
      "classification_rate: 0.8022222222222222\n",
      "classification_rate: 0.8588888888888889\n",
      "classification_rate: 0.8633333333333333\n",
      "classification_rate: 0.8466666666666667\n",
      "classification_rate: 0.8688888888888889\n",
      "classification_rate: 0.8666666666666667\n",
      "classification_rate: 0.8688888888888889\n",
      "classification_rate: 0.8522222222222222\n",
      "classification_rate: 0.8266666666666667\n",
      "classification_rate: 0.8822222222222222\n",
      "classification_rate: 0.8744444444444445\n",
      "classification_rate: 0.8744444444444445\n",
      "Test Set classification rate: 0.64\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.09111111111111111\n",
      "classification_rate: 0.22444444444444445\n",
      "classification_rate: 0.4111111111111111\n",
      "classification_rate: 0.55\n",
      "classification_rate: 0.6344444444444445\n",
      "classification_rate: 0.6244444444444445\n",
      "classification_rate: 0.6566666666666666\n",
      "classification_rate: 0.6355555555555555\n",
      "classification_rate: 0.6322222222222222\n",
      "classification_rate: 0.6988888888888889\n",
      "classification_rate: 0.7866666666666666\n",
      "classification_rate: 0.7866666666666666\n",
      "classification_rate: 0.8211111111111111\n",
      "classification_rate: 0.7255555555555555\n",
      "classification_rate: 0.8422222222222222\n",
      "classification_rate: 0.8533333333333334\n",
      "classification_rate: 0.8588888888888889\n",
      "classification_rate: 0.8477777777777777\n",
      "classification_rate: 0.7711111111111111\n",
      "classification_rate: 0.8533333333333334\n",
      "classification_rate: 0.8555555555555555\n",
      "classification_rate: 0.8711111111111111\n",
      "classification_rate: 0.8577777777777778\n",
      "classification_rate: 0.7933333333333333\n",
      "classification_rate: 0.8666666666666667\n",
      "classification_rate: 0.8677777777777778\n",
      "classification_rate: 0.8433333333333334\n",
      "classification_rate: 0.8755555555555555\n",
      "classification_rate: 0.8244444444444444\n",
      "Test Set classification rate: 0.6266666666666667\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.18888888888888888\n",
      "classification_rate: 0.18444444444444444\n",
      "classification_rate: 0.41444444444444445\n",
      "classification_rate: 0.4955555555555556\n",
      "classification_rate: 0.6033333333333334\n",
      "classification_rate: 0.6144444444444445\n",
      "classification_rate: 0.6455555555555555\n",
      "classification_rate: 0.7066666666666667\n",
      "classification_rate: 0.7322222222222222\n",
      "classification_rate: 0.7533333333333333\n",
      "classification_rate: 0.7622222222222222\n",
      "classification_rate: 0.8033333333333333\n",
      "classification_rate: 0.7866666666666666\n",
      "classification_rate: 0.8122222222222222\n",
      "classification_rate: 0.8455555555555555\n",
      "classification_rate: 0.8455555555555555\n",
      "classification_rate: 0.8322222222222222\n",
      "classification_rate: 0.8533333333333334\n",
      "classification_rate: 0.8566666666666667\n",
      "classification_rate: 0.8588888888888889\n",
      "classification_rate: 0.8655555555555555\n",
      "classification_rate: 0.8677777777777778\n",
      "classification_rate: 0.8611111111111112\n",
      "classification_rate: 0.8633333333333333\n",
      "classification_rate: 0.8744444444444445\n",
      "classification_rate: 0.8666666666666667\n",
      "classification_rate: 0.8633333333333333\n",
      "classification_rate: 0.8733333333333333\n",
      "classification_rate: 0.8333333333333334\n",
      "classification_rate: 0.8211111111111111\n",
      "Test Set classification rate: 0.6\n",
      "mean: 0.6275555555555555\n",
      "-------------------------------------\n",
      "\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.12444444444444444\n",
      "classification_rate: 0.33111111111111113\n",
      "classification_rate: 0.43\n",
      "classification_rate: 0.42333333333333334\n",
      "classification_rate: 0.8066666666666666\n",
      "classification_rate: 0.8388888888888889\n",
      "classification_rate: 0.8577777777777778\n",
      "classification_rate: 0.8811111111111111\n",
      "classification_rate: 0.8722222222222222\n",
      "classification_rate: 0.8766666666666667\n",
      "classification_rate: 0.8866666666666667\n",
      "classification_rate: 0.9\n",
      "classification_rate: 0.9188888888888889\n",
      "classification_rate: 0.9\n",
      "classification_rate: 0.5388888888888889\n",
      "classification_rate: 0.5522222222222222\n",
      "classification_rate: 0.8088888888888889\n",
      "classification_rate: 0.8911111111111111\n",
      "classification_rate: 0.9133333333333333\n",
      "classification_rate: 0.8066666666666666\n",
      "classification_rate: 0.9355555555555556\n",
      "classification_rate: 0.9311111111111111\n",
      "classification_rate: 0.9377777777777778\n",
      "classification_rate: 0.9488888888888889\n",
      "classification_rate: 0.9244444444444444\n",
      "classification_rate: 0.9411111111111111\n",
      "classification_rate: 0.9566666666666667\n",
      "classification_rate: 0.8011111111111111\n",
      "classification_rate: 0.9577777777777777\n",
      "classification_rate: 0.9622222222222222\n",
      "Test Set classification rate: 0.6866666666666666\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.16666666666666666\n",
      "classification_rate: 0.31555555555555553\n",
      "classification_rate: 0.4266666666666667\n",
      "classification_rate: 0.7188888888888889\n",
      "classification_rate: 0.7866666666666666\n",
      "classification_rate: 0.7866666666666666\n",
      "classification_rate: 0.8488888888888889\n",
      "classification_rate: 0.8788888888888889\n",
      "classification_rate: 0.8933333333333333\n",
      "classification_rate: 0.8922222222222222\n",
      "classification_rate: 0.9088888888888889\n",
      "classification_rate: 0.9066666666666666\n",
      "classification_rate: 0.9166666666666666\n",
      "classification_rate: 0.79\n",
      "classification_rate: 0.8844444444444445\n",
      "classification_rate: 0.8988888888888888\n",
      "classification_rate: 0.9144444444444444\n",
      "classification_rate: 0.9277777777777778\n",
      "classification_rate: 0.9322222222222222\n",
      "classification_rate: 0.9188888888888889\n",
      "classification_rate: 0.92\n",
      "classification_rate: 0.9433333333333334\n",
      "classification_rate: 0.9366666666666666\n",
      "classification_rate: 0.9466666666666667\n",
      "classification_rate: 0.9466666666666667\n",
      "classification_rate: 0.9522222222222222\n",
      "classification_rate: 0.93\n",
      "classification_rate: 0.92\n",
      "classification_rate: 0.9455555555555556\n",
      "classification_rate: 0.9555555555555556\n",
      "Test Set classification rate: 0.6666666666666666\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.11333333333333333\n",
      "classification_rate: 0.2311111111111111\n",
      "classification_rate: 0.2288888888888889\n",
      "classification_rate: 0.36444444444444446\n",
      "classification_rate: 0.41333333333333333\n",
      "classification_rate: 0.6611111111111111\n",
      "classification_rate: 0.8011111111111111\n",
      "classification_rate: 0.7355555555555555\n",
      "classification_rate: 0.8622222222222222\n",
      "classification_rate: 0.8855555555555555\n",
      "classification_rate: 0.88\n",
      "classification_rate: 0.8855555555555555\n",
      "classification_rate: 0.8855555555555555\n",
      "classification_rate: 0.8977777777777778\n",
      "classification_rate: 0.91\n",
      "classification_rate: 0.9166666666666666\n",
      "classification_rate: 0.66\n",
      "classification_rate: 0.9355555555555556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_rate: 0.9422222222222222\n",
      "classification_rate: 0.94\n",
      "classification_rate: 0.9311111111111111\n",
      "classification_rate: 0.9455555555555556\n",
      "classification_rate: 0.94\n",
      "classification_rate: 0.9033333333333333\n",
      "classification_rate: 0.9466666666666667\n",
      "classification_rate: 0.9255555555555556\n",
      "classification_rate: 0.9544444444444444\n",
      "classification_rate: 0.9188888888888889\n",
      "classification_rate: 0.73\n",
      "classification_rate: 0.9122222222222223\n",
      "Test Set classification rate: 0.6733333333333333\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.2222222222222222\n",
      "classification_rate: 0.42777777777777776\n",
      "classification_rate: 0.42444444444444446\n",
      "classification_rate: 0.5233333333333333\n",
      "classification_rate: 0.8211111111111111\n",
      "classification_rate: 0.8033333333333333\n",
      "classification_rate: 0.8533333333333334\n",
      "classification_rate: 0.8755555555555555\n",
      "classification_rate: 0.8422222222222222\n",
      "classification_rate: 0.9011111111111111\n",
      "classification_rate: 0.9033333333333333\n",
      "classification_rate: 0.7955555555555556\n",
      "classification_rate: 0.8844444444444445\n",
      "classification_rate: 0.9122222222222223\n",
      "classification_rate: 0.9211111111111111\n",
      "classification_rate: 0.9244444444444444\n",
      "classification_rate: 0.8788888888888889\n",
      "classification_rate: 0.9266666666666666\n",
      "classification_rate: 0.9233333333333333\n",
      "classification_rate: 0.9322222222222222\n",
      "classification_rate: 0.9566666666666667\n",
      "classification_rate: 0.9355555555555556\n",
      "classification_rate: 0.9477777777777778\n",
      "classification_rate: 0.9344444444444444\n",
      "classification_rate: 0.9811111111111112\n",
      "classification_rate: 0.9222222222222223\n",
      "classification_rate: 0.96\n",
      "classification_rate: 0.9588888888888889\n",
      "classification_rate: 0.9677777777777777\n",
      "Test Set classification rate: 0.6755555555555556\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.1111111111111111\n",
      "classification_rate: 0.18777777777777777\n",
      "classification_rate: 0.30444444444444446\n",
      "classification_rate: 0.4177777777777778\n",
      "classification_rate: 0.6222222222222222\n",
      "classification_rate: 0.6444444444444445\n",
      "classification_rate: 0.7466666666666667\n",
      "classification_rate: 0.82\n",
      "classification_rate: 0.8366666666666667\n",
      "classification_rate: 0.8588888888888889\n",
      "classification_rate: 0.8666666666666667\n",
      "classification_rate: 0.87\n",
      "classification_rate: 0.9011111111111111\n",
      "classification_rate: 0.9155555555555556\n",
      "classification_rate: 0.8888888888888888\n",
      "classification_rate: 0.9\n",
      "classification_rate: 0.5833333333333334\n",
      "classification_rate: 0.8344444444444444\n",
      "classification_rate: 0.8477777777777777\n",
      "classification_rate: 0.8844444444444445\n",
      "classification_rate: 0.8811111111111111\n",
      "classification_rate: 0.3811111111111111\n",
      "classification_rate: 0.77\n",
      "classification_rate: 0.8277777777777777\n",
      "classification_rate: 0.8277777777777777\n",
      "classification_rate: 0.86\n",
      "classification_rate: 0.11222222222222222\n",
      "classification_rate: 0.3566666666666667\n",
      "classification_rate: 0.35777777777777775\n",
      "classification_rate: 0.4533333333333333\n",
      "Test Set classification rate: 0.41555555555555557\n",
      "mean: 0.6235555555555555\n",
      "-------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset_id in range(3):\n",
    "    train = np.concatenate(preprocessed_datasets[dataset_id][0:2])\n",
    "    test = preprocessed_datasets[dataset_id][2]\n",
    "\n",
    "    Ytrain = train[:,-1].astype('int')\n",
    "    Xtrain = train[:,:-1]\n",
    "\n",
    "    Ytest = test[:,-1].astype('int')\n",
    "    Xtest = test[:,:-1]\n",
    "        \n",
    "    results_nn = []\n",
    "    for i in range(5):\n",
    "\n",
    "        NUM_READS = 1 # each row contains 1 read\n",
    "        NUM_SENSORS = 88 # each read has 8 sensors(channels) * 5 features\n",
    "\n",
    "        D = NUM_SENSORS * NUM_READS # number of input features\n",
    "        M1 = 25 # first layer number of nodes, relatively arbitrarily chosen\n",
    "        M2 = 15 # second hidden layer number of nodes, relatively arbitrarily chosen\n",
    "        M3 = 10 # third hidden layer number of nodes, relatively arbitrarily chosen\n",
    "\n",
    "        K = 9 # output layer nodes or number of classes\n",
    "\n",
    "        N = len(Ytrain)\n",
    "        T = np.zeros((N, K))\n",
    "        for i in range(N):\n",
    "            T[i, Ytrain[i]] = 1 # this creates an indicator/dummy variable matrix for the output layer. We need to do this for\n",
    "        # two reasons. 1) it creates an NxK matrix that will be broadcastable with the predictions generated from the forward\n",
    "        # function and used in the cost function. 2) when we argmax the predictions, it will turn into a matrix NxK of values only\n",
    "        # either 1 or 0 which can directly be compared with T to test the accuracy\n",
    "\n",
    "\n",
    "        def initialize_weights_and_biases(shape):\n",
    "            return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "        def feed_forward(W4, W3, W2, W1, b4, b3, b2, b1, X):\n",
    "            Z1 = tf.matmul(X, W1)\n",
    "        #     Z1 = tf.nn.dropout(Z1, 0.9) #check TFLite support\n",
    "            Z1 = tf.nn.relu(Z1 + b1)\n",
    "\n",
    "            Z2 = tf.matmul(Z1, W2)\n",
    "        #     Z2 = tf.nn.dropout(Z2, 0.9) #check TFLite support\n",
    "            Z2 = tf.nn.relu(Z2 + b2)\n",
    "\n",
    "            Z3 = tf.nn.relu(tf.matmul(Z2, W3) + b3)\n",
    "            return tf.matmul(Z3, W4) + b4, Z3\n",
    "\n",
    "        tfX = tf.placeholder(tf.float32, [None, D]) # creates placeholder variables without actually assigning values to them yet\n",
    "        tfY = tf.placeholder(tf.float32, [None, K]) # None means it can take any size N total number of instances\n",
    "\n",
    "\n",
    "        W1 = initialize_weights_and_biases([D, M1])\n",
    "        W2 = initialize_weights_and_biases([M1, M2])\n",
    "        W3 = initialize_weights_and_biases([M2, M3])\n",
    "        W4 = initialize_weights_and_biases([M3, K])\n",
    "        b1 = initialize_weights_and_biases([M1])\n",
    "        b2 = initialize_weights_and_biases([M2])\n",
    "        b3 = initialize_weights_and_biases([M3])\n",
    "        b4 = initialize_weights_and_biases([K])\n",
    "\n",
    "        pY_given_X, mid_layer = feed_forward(W4, W3, W2, W1, b4, b3, b2, b1, tfX)\n",
    "        y_max = tf.argmax(pY_given_X, dimension=1)\n",
    "\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels = tfY, logits = pY_given_X))\n",
    "\n",
    "        train_model = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "        predict_output = tf.argmax(pY_given_X, 1) # 1 refers to axis = 1, meaning it does argmax on each instance n\n",
    "\n",
    "        session = tf.Session()\n",
    "        initializer = tf.global_variables_initializer()\n",
    "        session.run(initializer)\n",
    "\n",
    "        for i in range(8000):\n",
    "            session.run(train_model, feed_dict = {tfX: Xtrain, tfY: T})\n",
    "            pred = session.run(predict_output, feed_dict = {tfX:Xtrain, tfY:T})\n",
    "            if i % 250 == 0:\n",
    "                print(\"classification_rate: {}\".format(np.mean(Ytrain == pred)))\n",
    "\n",
    "        # Test Set evaluation\n",
    "        Ntest = len(Ytest)\n",
    "        Ttest = np.zeros((Ntest, K)) # test set indicator matrix\n",
    "        for i in range(Ntest):\n",
    "            Ttest[i, Ytest[i]] = 1\n",
    "\n",
    "        predtest = session.run(predict_output, feed_dict = {tfX: Xtest, tfY: Ttest})\n",
    "        acc = np.mean(Ytest == predtest)\n",
    "        results_nn.append(acc)\n",
    "        print(\"Test Set classification rate: {}\".format(acc))\n",
    "    print('mean: {}'.format(np.mean(results_nn)))\n",
    "    print('-------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
